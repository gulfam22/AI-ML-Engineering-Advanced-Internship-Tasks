{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K3CNpHrc8zs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f6519c8",
        "outputId": "870ec20e-598d-4bd9-e341-c680e3399a27"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the tabular data\n",
        "df = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "\n",
        "# Identify missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values before handling:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Display DataFrame info\n",
        "df.info()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values before handling:\n",
            "Series([], dtype: int64)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17000 entries, 0 to 16999\n",
            "Data columns (total 9 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   longitude           17000 non-null  float64\n",
            " 1   latitude            17000 non-null  float64\n",
            " 2   housing_median_age  17000 non-null  float64\n",
            " 3   total_rooms         17000 non-null  float64\n",
            " 4   total_bedrooms      17000 non-null  float64\n",
            " 5   population          17000 non-null  float64\n",
            " 6   households          17000 non-null  float64\n",
            " 7   median_income       17000 non-null  float64\n",
            " 8   median_house_value  17000 non-null  float64\n",
            "dtypes: float64(9)\n",
            "memory usage: 1.2 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cad1a97"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load the data failed due to a FileNotFoundError. I need to use the correct file path for the housing data. Based on the context, the correct path is likely `/data/housing.csv`. I will try loading the data again with the corrected path and check for missing values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpSVl7O2e5dM"
      },
      "source": [
        "# Load the tabular data with the correct path\n",
        "df = pd.read_csv('/data/housing.csv')\n",
        "\n",
        "# Identify missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values before handling:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Display DataFrame info\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9657e8c1"
      },
      "source": [
        "**Reasoning**:\n",
        "The data file was not found in the specified path. I need to check the available files in the environment to find the correct path for the housing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMyR_5S_e7kA"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content'))\n",
        "print(os.listdir('/content/sample_data'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4226869c"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the file listing, the housing data is likely in the `/content/sample_data` directory and named `california_housing_train.csv`. I will load this file, check for missing values, impute them, identify categorical and numerical features, and scale the numerical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e439f29f"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the tabular data\n",
        "df = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "\n",
        "# Identify missing values\n",
        "print(\"Missing values before handling:\")\n",
        "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "\n",
        "# Separate target variable\n",
        "X = df.drop('median_house_value', axis=1)\n",
        "y = df['median_house_value']\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "numerical_features = X.select_dtypes(include=np.number).columns\n",
        "\n",
        "# Create preprocessing pipelines for numerical and categorical features\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Create a column transformer to apply different transformations to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(\"\\nMissing values after handling and preprocessing:\")\n",
        "# Convert to DataFrame to easily check for NaNs after preprocessing if needed, though ColumnTransformer handles this.\n",
        "# Note: ColumnTransformer output can be sparse, so converting to dense array first for sum()\n",
        "if hasattr(X_preprocessed, 'toarray'):\n",
        "    print(np.isnan(X_preprocessed.toarray()).sum())\n",
        "else:\n",
        "    print(np.isnan(X_preprocessed).sum())\n",
        "\n",
        "print(\"\\nShape of preprocessed features:\", X_preprocessed.shape)\n",
        "print(\"Shape of target variable:\", y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21a289a4"
      },
      "source": [
        "## Load and preprocess image data\n",
        "\n",
        "### Subtask:\n",
        "Load the house images and perform necessary preprocessing steps such as resizing, normalization, and data augmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eba2a43"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to load and preprocess the house images. Since the location of the images is not provided, I will first check the available directories to see if I can find them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17afc4f",
        "outputId": "d11ed74b-f0d9-4ae5-e42b-61d2046e6520"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content'))\n",
        "print(os.listdir('/content/sample_data'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'dummy_house_images', 'sample_data']\n",
            "['README.md', 'anscombe.json', 'california_housing_test.csv', 'mnist_test.csv', 'california_housing_train.csv', 'mnist_train_small.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb499c2"
      },
      "source": [
        "## Load and preprocess image data\n",
        "\n",
        "### Subtask:\n",
        "Load the house images and perform necessary preprocessing steps such as resizing, normalization, and data augmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd99624c"
      },
      "source": [
        "## Load and preprocess image data - retry 1\n",
        "\n",
        "### Subtask:\n",
        "Load the house images and perform necessary preprocessing steps such as resizing, normalization, and data augmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4337be72"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a dummy directory structure and dummy image files to simulate the presence of image data, as the actual images are not available. Then load, resize, and normalize these dummy images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8968b54",
        "outputId": "f362f4a7-4552-47b4-a259-91b0d157211d"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Create dummy directory structure and dummy image files\n",
        "image_dir = '/content/dummy_house_images'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# Assuming the number of housing records is the number of rows in the dataframe X\n",
        "num_records = X.shape[0]\n",
        "dummy_image_size = (64, 64) # Define a consistent size for dummy images\n",
        "\n",
        "dummy_image_paths = []\n",
        "for i in range(num_records):\n",
        "    # Create a dummy image file for each record\n",
        "    dummy_image_path = os.path.join(image_dir, f'house_{i:05d}.png')\n",
        "    dummy_image = Image.fromarray(np.random.randint(0, 256, (*dummy_image_size, 3), dtype=np.uint8), 'RGB')\n",
        "    dummy_image.save(dummy_image_path)\n",
        "    dummy_image_paths.append(dummy_image_path)\n",
        "\n",
        "print(f\"Created {num_records} dummy images in {image_dir}\")\n",
        "\n",
        "# 2. Load the dummy image files\n",
        "loaded_images = []\n",
        "for img_path in dummy_image_paths:\n",
        "    img = Image.open(img_path)\n",
        "    loaded_images.append(img)\n",
        "\n",
        "print(f\"Loaded {len(loaded_images)} dummy images\")\n",
        "\n",
        "# 3. Resize the loaded images to a consistent size (already done during creation, but good practice to include)\n",
        "# This step is implicitly handled by creating images of dummy_image_size, but if loading existing images,\n",
        "# you would resize here.\n",
        "resized_images = [img.resize(dummy_image_size) for img in loaded_images]\n",
        "print(f\"Resized {len(resized_images)} dummy images to {dummy_image_size}\")\n",
        "\n",
        "# 4. Normalize the pixel values\n",
        "# Convert PIL images to NumPy arrays first\n",
        "image_arrays = [np.array(img) for img in resized_images]\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "normalized_images = [(img_array / 255.0) for img_array in image_arrays]\n",
        "print(f\"Normalized {len(normalized_images)} dummy images\")\n",
        "\n",
        "# 5. Consider applying basic data augmentation techniques (Optional for this subtask, but can be added here)\n",
        "# For simplicity in this subtask, we will skip augmentation.\n",
        "\n",
        "# 6. Store the preprocessed images in a NumPy array\n",
        "preprocessed_images_array = np.array(normalized_images)\n",
        "print(f\"Stored preprocessed images in a NumPy array with shape: {preprocessed_images_array.shape}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3842646937.py:17: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  dummy_image = Image.fromarray(np.random.randint(0, 256, (*dummy_image_size, 3), dtype=np.uint8), 'RGB')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 17000 dummy images in /content/dummy_house_images\n",
            "Loaded 17000 dummy images\n",
            "Resized 17000 dummy images to (64, 64)\n",
            "Normalized 17000 dummy images\n",
            "Stored preprocessed images in a NumPy array with shape: (17000, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6196cfb"
      },
      "source": [
        "## Build cnn model for image feature extraction\n",
        "\n",
        "### Subtask:\n",
        "Create a Convolutional Neural Network (CNN) model to extract features from the house images. This model will be a part of the overall multimodal model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3393c8bc"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary Keras layers and the Model class, define the input shape, build the CNN model using the functional API, and print the model summary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4012415c"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input shape for the CNN\n",
        "# The shape is (height, width, channels)\n",
        "image_input_shape = dummy_image_size + (3,) # dummy_image_size is (64, 64), channels is 3 for RGB\n",
        "\n",
        "# Define the input layer\n",
        "image_input = Input(shape=image_input_shape, name='image_input')\n",
        "\n",
        "# Build a very small CNN model using the Functional API\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(image_input) # Reduced filters\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x) # Reduced filters\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "# Add dense layers for feature extraction - Reduced neurons\n",
        "x = Dense(16, activation='relu')(x) # Reduced neurons\n",
        "image_features_output = Dense(8, activation='relu', name='image_features')(x) # Output layer for features, Reduced neurons\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Model(inputs=image_input, outputs=image_features_output)\n",
        "\n",
        "# Print the model summary\n",
        "cnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8efb9f7"
      },
      "source": [
        "## Combine tabular and image features\n",
        "\n",
        "### Subtask:\n",
        "Concatenate the extracted image features from the CNN with the preprocessed tabular data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d9382c7"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the input layer for the tabular data, get the output tensor from the CNN model, concatenate the two inputs, and print the shape of the concatenated layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "647b8440"
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define an input layer for the tabular data.\n",
        "# The shape should match the number of features in X_preprocessed.\n",
        "tabular_input_shape = (X_preprocessed.shape[1],)\n",
        "tabular_input = Input(shape=tabular_input_shape, name='tabular_input')\n",
        "\n",
        "# 2. Get the output tensor from the previously built cnn_model.\n",
        "image_features_output = cnn_model.output\n",
        "\n",
        "# 3. Concatenate the tabular input layer and the image features output tensor.\n",
        "concatenated_features = Concatenate()([tabular_input, image_features_output])\n",
        "\n",
        "# 4. Print the shape of the concatenated features.\n",
        "print(\"Shape of concatenated features:\", concatenated_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53d0cd62"
      },
      "source": [
        "## Build and train multimodal model\n",
        "\n",
        "### Subtask:\n",
        "Create a model that takes both the combined features as input and predicts the housing price. Train this model on the prepared dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a107d8c3"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the final multimodal model, compile it, and train it using the prepared data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1a251df"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError\n",
        "import numpy as np\n",
        "\n",
        "# Define the output layer for the combined model.\n",
        "# This layer should have a single neuron with no activation function for regression.\n",
        "output_layer = Dense(1, name='output')\n",
        "\n",
        "# Define a new Input layer for the image data in the multimodal model\n",
        "multimodal_image_input = Input(shape=image_input_shape, name='multimodal_image_input')\n",
        "\n",
        "# Get the output of the cnn_model when fed with the multimodal_image_input\n",
        "cnn_output_for_multimodal = cnn_model(multimodal_image_input)\n",
        "\n",
        "# Concatenate the tabular input and the output of the CNN for the multimodal model\n",
        "multimodal_concatenated_features = Concatenate()([tabular_input, cnn_output_for_multimodal])\n",
        "\n",
        "# Connect the concatenated features to the output layer.\n",
        "predictions = Dense(1, name='output')(multimodal_concatenated_features)\n",
        "\n",
        "\n",
        "# Create the final multimodal model using the Keras Model API.\n",
        "# Specify the inputs (tabular input and the new multimodal image input) and the output.\n",
        "multimodal_model = Model(inputs=[tabular_input, multimodal_image_input], outputs=predictions)\n",
        "\n",
        "# Compile the multimodal model.\n",
        "# Specify an optimizer (e.g., 'adam'), a loss function suitable for regression (e.g., 'mean_squared_error' or 'mae'),\n",
        "# and evaluation metrics (e.g., ['mae', 'mse', 'rmse'] if RMSE is available).\n",
        "# Keras includes MAE and MSE. RMSE is the square root of MSE. We can calculate RMSE manually if needed,\n",
        "# but for compilation, MAE and MSE are standard.\n",
        "multimodal_model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                         loss='mean_squared_error', # Using MSE as the primary loss\n",
        "                         metrics=[MeanAbsoluteError(), MeanSquaredError()]) # Including MAE and MSE as metrics\n",
        "\n",
        "# Print the model summary\n",
        "multimodal_model.summary()\n",
        "\n",
        "# Train the multimodal model using the fit() method.\n",
        "# Provide the preprocessed tabular data (X_preprocessed), the preprocessed image data (preprocessed_images_array),\n",
        "# and the target variable (y). Specify the number of epochs and batch size.\n",
        "# Consider using a validation split or a separate validation set to monitor performance during training.\n",
        "\n",
        "# Reshape X_preprocessed to be 2D if it's not already (ColumnTransformer output is typically 2D)\n",
        "if X_preprocessed.ndim == 1:\n",
        "    X_preprocessed = X_preprocessed.reshape(-1, 1)\n",
        "elif X_preprocessed.ndim > 2:\n",
        "     # Handle cases where preprocessing might lead to >2 dims unexpectedly\n",
        "     X_preprocessed = X_preprocessed.reshape(X_preprocessed.shape[0], -1)\n",
        "\n",
        "\n",
        "print(\"\\nTraining the multimodal model...\")\n",
        "history = multimodal_model.fit(\n",
        "    [X_preprocessed, preprocessed_images_array], # List of inputs\n",
        "    y,                                         # Target variable\n",
        "    epochs=5,                                 # Reduced epochs for quick check\n",
        "    batch_size=64,                             # Increased batch size for faster training\n",
        "    validation_split=0.2                       # Use 20% of data for validation\n",
        ")\n",
        "\n",
        "print(\"\\nMultimodal model training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71bb0b78"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model using appropriate metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e7bd470"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained multimodal model on the validation data to assess its performance using MAE and RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac997cee"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "# The validation data is automatically handled by the validation_split in model.fit\n",
        "# We can access the evaluation metrics from the history object\n",
        "\n",
        "print(\"\\nEvaluating the multimodal model on the validation data...\")\n",
        "\n",
        "# The history object contains the metrics calculated during training, including validation metrics\n",
        "# The last epoch's validation metrics are typically the most relevant for evaluation\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_mae = history.history['val_mean_absolute_error'][-1]\n",
        "val_mse = history.history['val_mean_squared_error'][-1]\n",
        "val_rmse = np.sqrt(val_mse) # Calculate RMSE from MSE\n",
        "\n",
        "print(f\"Validation Loss (MSE): {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "print(\"\\nMultimodal model evaluation finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52330d52"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model using appropriate metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1efb9d2"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained multimodal model on the validation data to assess its performance using MAE and RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c94bf92"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "# The validation data is automatically handled by the validation_split in model.fit\n",
        "# We can access the evaluation metrics from the history object\n",
        "\n",
        "print(\"\\nEvaluating the multimodal model on the validation data...\")\n",
        "\n",
        "# The history object contains the metrics calculated during training, including validation metrics\n",
        "# The last epoch's validation metrics are typically the most relevant for evaluation\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_mae = history.history['val_mean_absolute_error'][-1]\n",
        "val_mse = history.history['val_mean_squared_error'][-1]\n",
        "val_rmse = np.sqrt(val_mse) # Calculate RMSE from MSE\n",
        "\n",
        "print(f\"Validation Loss (MSE): {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "print(\"\\nMultimodal model evaluation finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95904ed9"
      },
      "source": [
        "## Load and preprocess image data\n",
        "\n",
        "### Subtask:\n",
        "Load the house images and perform necessary preprocessing steps such as resizing, normalization, and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10596668"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to load and preprocess the house images. Since the location of the images is not provided, I will first check the available directories to see if I can find them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24368cfb",
        "outputId": "dbbfd426-50a0-4be5-a3ea-e7e93cf063e3"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content'))\n",
        "print(os.listdir('/content/sample_data'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'dummy_house_images', 'sample_data']\n",
            "['README.md', 'anscombe.json', 'california_housing_test.csv', 'mnist_test.csv', 'california_housing_train.csv', 'mnist_train_small.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ebcea52"
      },
      "source": [
        "## Load and preprocess image data\n",
        "\n",
        "### Subtask:\n",
        "Load the house images and perform necessary preprocessing steps such as resizing, normalization, and data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933258b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a dummy directory structure and dummy image files to simulate the presence of image data, as the actual images are not available. Then load, resize, and normalize these dummy images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a81d911",
        "outputId": "3998038e-2117-4db5-f740-bda3d31dfefa"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Create dummy directory structure and dummy image files\n",
        "image_dir = '/content/dummy_house_images'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "\n",
        "# Assuming the number of housing records is the number of rows in the dataframe X\n",
        "num_records = X.shape[0]\n",
        "dummy_image_size = (64, 64) # Define a consistent size for dummy images\n",
        "\n",
        "dummy_image_paths = []\n",
        "for i in range(num_records):\n",
        "    # Create a dummy image file for each record\n",
        "    dummy_image_path = os.path.join(image_dir, f'house_{i:05d}.png')\n",
        "    dummy_image = Image.fromarray(np.random.randint(0, 256, (*dummy_image_size, 3), dtype=np.uint8), 'RGB')\n",
        "    dummy_image.save(dummy_image_path)\n",
        "    dummy_image_paths.append(dummy_image_path)\n",
        "\n",
        "print(f\"Created {num_records} dummy images in {image_dir}\")\n",
        "\n",
        "# 2. Load the dummy image files\n",
        "loaded_images = []\n",
        "for img_path in dummy_image_paths:\n",
        "    img = Image.open(img_path)\n",
        "    loaded_images.append(img)\n",
        "\n",
        "print(f\"Loaded {len(loaded_images)} dummy images\")\n",
        "\n",
        "# 3. Resize the loaded images to a consistent size (already done during creation, but good practice to include)\n",
        "# This step is implicitly handled by creating images of dummy_image_size, but if loading existing images,\n",
        "# you would resize here.\n",
        "resized_images = [img.resize(dummy_image_size) for img in loaded_images]\n",
        "print(f\"Resized {len(resized_images)} dummy images to {dummy_image_size}\")\n",
        "\n",
        "# 4. Normalize the pixel values\n",
        "# Convert PIL images to NumPy arrays first\n",
        "image_arrays = [np.array(img) for img in resized_images]\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "normalized_images = [(img_array / 255.0) for img_array in image_arrays]\n",
        "print(f\"Normalized {len(normalized_images)} dummy images\")\n",
        "\n",
        "# 5. Consider applying basic data augmentation techniques (Optional for this subtask, but can be added here)\n",
        "# For simplicity in this subtask, we will skip augmentation.\n",
        "\n",
        "# 6. Store the preprocessed images in a NumPy array\n",
        "preprocessed_images_array = np.array(normalized_images)\n",
        "print(f\"Stored preprocessed images in a NumPy array with shape: {preprocessed_images_array.shape}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3842646937.py:17: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  dummy_image = Image.fromarray(np.random.randint(0, 256, (*dummy_image_size, 3), dtype=np.uint8), 'RGB')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 17000 dummy images in /content/dummy_house_images\n",
            "Loaded 17000 dummy images\n",
            "Resized 17000 dummy images to (64, 64)\n",
            "Normalized 17000 dummy images\n",
            "Stored preprocessed images in a NumPy array with shape: (17000, 64, 64, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e641c8d8"
      },
      "source": [
        "## Build cnn model for image feature extraction\n",
        "\n",
        "### Subtask:\n",
        "Create a Convolutional Neural Network (CNN) model to extract features from the house images. This model will be a part of the overall multimodal model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "949f049f"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary Keras layers and the Model class, define the input shape, build the CNN model using the functional API, and print the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "40a6932e",
        "outputId": "0f48d095-fcbb-4713-f915-8eca359ef04c"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input shape for the CNN\n",
        "# The shape is (height, width, channels)\n",
        "image_input_shape = dummy_image_size + (3,) # dummy_image_size is (64, 64), channels is 3 for RGB\n",
        "\n",
        "# Define the input layer\n",
        "image_input = Input(shape=image_input_shape, name='image_input')\n",
        "\n",
        "# Build a very small CNN model using the Functional API\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(image_input) # Reduced filters\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x) # Reduced filters\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Flatten()(x)\n",
        "# Add dense layers for feature extraction - Reduced neurons\n",
        "x = Dense(16, activation='relu')(x) # Reduced neurons\n",
        "image_features_output = Dense(8, activation='relu', name='image_features')(x) # Output layer for features, Reduced neurons\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Model(inputs=image_input, outputs=image_features_output)\n",
        "\n",
        "# Print the model summary\n",
        "cnn_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m224\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m1,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │        \u001b[38;5;34m65,552\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ image_features (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,552</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ image_features (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,080\u001b[0m (262.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,080</span> (262.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,080\u001b[0m (262.03 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,080</span> (262.03 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ea71362"
      },
      "source": [
        "## Combine tabular and image features\n",
        "\n",
        "### Subtask:\n",
        "Concatenate the extracted image features from the CNN with the preprocessed tabular data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f685dc32"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the input layer for the tabular data, get the output tensor from the CNN model, concatenate the two inputs, and print the shape of the concatenated layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d8e73d1",
        "outputId": "d8711823-c231-49e4-ea3a-b5320e54b50e"
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate, Input\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Define an input layer for the tabular data.\n",
        "# The shape should match the number of features in X_preprocessed.\n",
        "tabular_input_shape = (X_preprocessed.shape[1],)\n",
        "tabular_input = Input(shape=tabular_input_shape, name='tabular_input')\n",
        "\n",
        "# 2. Get the output tensor from the previously built cnn_model.\n",
        "image_features_output = cnn_model.output\n",
        "\n",
        "# 3. Concatenate the tabular input layer and the image features output tensor.\n",
        "concatenated_features = Concatenate()([tabular_input, image_features_output])\n",
        "\n",
        "# 4. Print the shape of the concatenated features.\n",
        "print(\"Shape of concatenated features:\", concatenated_features.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of concatenated features: (None, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e67af53"
      },
      "source": [
        "## Build and train multimodal model\n",
        "\n",
        "### Subtask:\n",
        "Create a model that takes both the combined features as input and predicts the housing price. Train this model on the prepared dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ece832b"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the final multimodal model, compile it, and train it using the prepared data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "1000043c",
        "outputId": "7314400c-95da-4d57-e30e-2861bc84742f"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError, MeanSquaredError\n",
        "import numpy as np\n",
        "\n",
        "# Define the output layer for the combined model.\n",
        "# This layer should have a single neuron with no activation function for regression.\n",
        "output_layer = Dense(1, name='output')\n",
        "\n",
        "# Define a new Input layer for the image data in the multimodal model\n",
        "multimodal_image_input = Input(shape=image_input_shape, name='multimodal_image_input')\n",
        "\n",
        "# Get the output of the cnn_model when fed with the multimodal_image_input\n",
        "cnn_output_for_multimodal = cnn_model(multimodal_image_input)\n",
        "\n",
        "# Concatenate the tabular input and the output of the CNN for the multimodal model\n",
        "multimodal_concatenated_features = Concatenate()([tabular_input, cnn_output_for_multimodal])\n",
        "\n",
        "# Connect the concatenated features to the output layer.\n",
        "predictions = Dense(1, name='output')(multimodal_concatenated_features)\n",
        "\n",
        "\n",
        "# Create the final multimodal model using the Keras Model API.\n",
        "# Specify the inputs (tabular input and the new multimodal image input) and the output.\n",
        "multimodal_model = Model(inputs=[tabular_input, multimodal_image_input], outputs=predictions)\n",
        "\n",
        "# Compile the multimodal model.\n",
        "# Specify an optimizer (e.g., 'adam'), a loss function suitable for regression (e.g., 'mean_squared_error' or 'mae'),\n",
        "# and evaluation metrics (e.g., ['mae', 'mse', 'rmse'] if RMSE is available).\n",
        "# Keras includes MAE and MSE. RMSE is the square root of MSE. We can calculate RMSE manually if needed,\n",
        "# but for compilation, MAE and MSE are standard.\n",
        "multimodal_model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                         loss='mean_squared_error', # Using MSE as the primary loss\n",
        "                         metrics=[MeanAbsoluteError(), MeanSquaredError()]) # Including MAE and MSE as metrics\n",
        "\n",
        "# Print the model summary\n",
        "multimodal_model.summary()\n",
        "\n",
        "# Train the multimodal model using the fit() method.\n",
        "# Provide the preprocessed tabular data (X_preprocessed), the preprocessed image data (preprocessed_images_array),\n",
        "# and the target variable (y). Specify the number of epochs and batch size.\n",
        "# Consider using a validation split or a separate validation set to monitor performance during training.\n",
        "\n",
        "# Reshape X_preprocessed to be 2D if it's not already (ColumnTransformer output is typically 2D)\n",
        "if X_preprocessed.ndim == 1:\n",
        "    X_preprocessed = X_preprocessed.reshape(-1, 1)\n",
        "elif X_preprocessed.ndim > 2:\n",
        "     # Handle cases where preprocessing might lead to >2 dims unexpectedly\n",
        "     X_preprocessed = X_preprocessed.reshape(X_preprocessed.shape[0], -1)\n",
        "\n",
        "\n",
        "print(\"\\nTraining the multimodal model...\")\n",
        "history = multimodal_model.fit(\n",
        "    [X_preprocessed, preprocessed_images_array], # List of inputs\n",
        "    y,                                         # Target variable\n",
        "    epochs=5,                                 # Reduced epochs for quick check\n",
        "    batch_size=64,                             # Increased batch size for faster training\n",
        "    validation_split=0.2                       # Use 20% of data for validation\n",
        ")\n",
        "\n",
        "print(\"\\nMultimodal model training finished.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_10\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_10\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ multimodal_image_i… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tabular_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │     \u001b[38;5;34m67,080\u001b[0m │ multimodal_image… │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ tabular_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ functional_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m17\u001b[0m │ concatenate_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ multimodal_image_i… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ tabular_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ functional_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">67,080</span> │ multimodal_image… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ tabular_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ functional_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m67,097\u001b[0m (262.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,097</span> (262.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m67,097\u001b[0m (262.10 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,097</span> (262.10 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the multimodal model...\n",
            "Epoch 1/5\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 136ms/step - loss: 45600907264.0000 - mean_absolute_error: 180862.8594 - mean_squared_error: 45600907264.0000 - val_loss: 18403620864.0000 - val_mean_absolute_error: 106981.3359 - val_mean_squared_error: 18403620864.0000\n",
            "Epoch 2/5\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 142ms/step - loss: 12255005696.0000 - mean_absolute_error: 86098.4062 - mean_squared_error: 12255005696.0000 - val_loss: 18758090752.0000 - val_mean_absolute_error: 107592.5703 - val_mean_squared_error: 18758090752.0000\n",
            "Epoch 3/5\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 130ms/step - loss: 12271631360.0000 - mean_absolute_error: 85865.9922 - mean_squared_error: 12271631360.0000 - val_loss: 18489327616.0000 - val_mean_absolute_error: 107127.8984 - val_mean_squared_error: 18489327616.0000\n",
            "Epoch 4/5\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 132ms/step - loss: 12146786304.0000 - mean_absolute_error: 85266.0312 - mean_squared_error: 12146786304.0000 - val_loss: 18888353792.0000 - val_mean_absolute_error: 107822.5312 - val_mean_squared_error: 18888353792.0000\n",
            "Epoch 5/5\n",
            "\u001b[1m213/213\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 132ms/step - loss: 12429488128.0000 - mean_absolute_error: 86262.4219 - mean_squared_error: 12429488128.0000 - val_loss: 19411025920.0000 - val_mean_absolute_error: 108802.1094 - val_mean_squared_error: 19411025920.0000\n",
            "\n",
            "Multimodal model training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "108003bc"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model using appropriate metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ab86572"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained multimodal model on the validation data to assess its performance using MAE and RMSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cc9415d",
        "outputId": "5f7bf87c-bf03-480e-c4d7-d76c099a051c"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "# The validation data is automatically handled by the validation_split in model.fit\n",
        "# We can access the evaluation metrics from the history object\n",
        "\n",
        "print(\"\\nEvaluating the multimodal model on the validation data...\")\n",
        "\n",
        "# The history object contains the metrics calculated during training, including validation metrics\n",
        "# The last epoch's validation metrics are typically the most relevant for evaluation\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_mae = history.history['val_mean_absolute_error'][-1]\n",
        "val_mse = history.history['val_mean_squared_error'][-1]\n",
        "val_rmse = np.sqrt(val_mse) # Calculate RMSE from MSE\n",
        "\n",
        "print(f\"Validation Loss (MSE): {val_loss:.4f}\")\n",
        "print(f\"Validation MAE: {val_mae:.4f}\")\n",
        "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "print(\"\\nMultimodal model evaluation finished.\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating the multimodal model on the validation data...\n",
            "Validation Loss (MSE): 19411025920.0000\n",
            "Validation MAE: 108802.1094\n",
            "Validation RMSE: 139323.4579\n",
            "\n",
            "Multimodal model evaluation finished.\n"
          ]
        }
      ]
    }
  ]
}